---
title: How I started my testing journey
author: Nathaniel Ward
layout: post
permalink: /2015/11/how-i-started-my-testing-journey/
categories:
  - Marketing
tags:
  - A/B testing
  - Optimization
  - Testing
  - Tim Kachuriak
---
I had a big problem.

The online fundraising campaign I was running had lost steam. Instead of generating thousands of dollars a day, it was generating just hundreds.

Traffic to the website, driven largely by advertising outside of our control, had fallen off sharply. But the expectations about how much money we’d make from the site hadn’t changed at all.

In short, we had to make more money from fewer website visitors. I knew something needed to change.

My first instinct was to test different elements on the page. With a little skill and a little luck, we would find a technique that would make more money and offset the decline in traffic.

Some of our early tests showed promise. For example, we found that we could boost revenue by changing the gift amount we asked for. We also found that adding an email signup option would (indirectly) get more people to donate.

But none of this made up for the decline in visitors and revenue, which was only accelerating.

## The stupidest idea I had ever heard

At this point, my friend and marketing co-conspirator Tim Kachuriak came to me with the stupidest idea I had ever heard. He had just been to a conference, and had really drunk the Kool-Aid. He was extremely excited, and that made me nervous.

His big idea was to test the existing page against a completely new version of the page:

  * Instead of putting the donation form right at the top of the page where potential donors could find it, he’d bury it at the bottom of the page.
  * Instead of including a short-and-sweet paragraph making the case for giving, he’d make page visitors wade through 700 words of copy, some of it repetitive.
  * Instead of using a page layout and color scheme that matched our branding, he’d use a bland, tan-colored page with just our logo on the top.
  * Instead of leading with the website name, to allow visitors to know where they were, he wanted a big long headline.

This “radical redesign” violated just about every fundraising and website best practice on the books.

Not only that, we’d be testing a huge number of changes at once. How would we know what caused the improvement?

I immediately told him he was a fool.

Tim persisted. He explained his rationale for the changes.

The people coming to the site may not yet be convinced that donating is for them, he explained, so we need to sell them before we ask for money. That means we need a lot more copy, to do the explaining. And that means the donation form has to go at the end of all that copy, after someone has bought in.

I wasn’t convinced, but Tim wore me down. I gave in—mostly to get him to shut up about the test already.

## Turns out, my gut instinct was totally wrong

It’s a good thing I eventually caved in and allowed Tim to try his harebrained idea.

Tim’s crazy, rule-breaking page performed better than the original. *Much* better.

*It brought in 74 percent more gifts* than the existing page. Not only that, *the average gift was 179 percent higher*.

Combine those together and *revenue from the new page was up a whopping 274 percent*. That’s more than triple the money.

## A new, more robust approach to testing

This result started me thinking about a new, more robust approach to testing.

In our old way of thinking, we tested one page element at a time. We would swap out one image, or one change the text on one button, or modify one headline. This approach had a major upside: because we changed just one thing at a time, we knew that any difference between versions was because of that change. But this approach was also scattershot, with no real rhyme or reason to it; it took a long time to get meaningful results; and we never learned enduring lessons.

In the new way of thinking, we would test not *individual elements* but rather comprehensive *theories of the donor*. So instead of testing a red button against a blue button, we would test a page addressed to one sort of donor against a page addressed to a different sort of donor.

This approach has several advantages, as you’ll see as soon as you try it:

  * **You learn a lot more**. Because you are testing fundamental concepts about our donors, you can extrapolate lessons from one donation page to dozens of others. You understand more about *why* a page works, not just that it does.
  * **You get test results a lot faster**. By combining several changes into a single test, you can usually find out awfully quickly whether your concept works or not.
  * **You have more fun**. In part because you are always challenging the conventional wisdom, running radical tests is awfully invigorating. You aren’t just blindly following someone else’s “best practices” and hoping they’ll work.

So what are you waiting for? Start testing!